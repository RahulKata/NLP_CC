{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>Different architectures</u></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><strong>Continuous Bag-Of-Words (CBOW) :</strong>attempts to guess the output (target word) from its neighbouring words (context words)</li><li><strong>Skip Gram :</strong>the input is the center word and the predictions are the context words.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/cbowvsskipgram.png\" alt=\"difference between word2vec architectures\" width=\"600\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's take a look at skip gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><h3>Word2Vec class </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import string \n",
    "from nltk.corpus import stopwords  \n",
    "   \n",
    "def softmax(x): \n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x)) \n",
    "    return e_x / e_x.sum() \n",
    "   \n",
    "class word2vec(object): \n",
    "    def __init__(self): \n",
    "        self.N = 10\n",
    "        self.X_train = [] \n",
    "        self.y_train = [] \n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.001\n",
    "        self.words = [] \n",
    "        self.word_index = {} \n",
    "   \n",
    "    def initialize(self,V,data): \n",
    "        self.V = V \n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
    "           \n",
    "        self.words = data \n",
    "        for i in range(len(data)): \n",
    "            self.word_index[data[i]] = i \n",
    "   \n",
    "       \n",
    "    def feed_forward(self,X): \n",
    "        self.h = np.dot(self.W.T,X).reshape(self.N,1) \n",
    "        self.u = np.dot(self.W1.T,self.h) \n",
    "        #print(self.u) \n",
    "        self.y = softmax(self.u)   \n",
    "        return self.y \n",
    "           \n",
    "    def backpropagate(self,x,t): \n",
    "        e = self.y - np.asarray(t).reshape(self.V,1) \n",
    "        # e.shape is V x 1 \n",
    "        dLdW1 = np.dot(self.h,e.T) \n",
    "        X = np.array(x).reshape(self.V,1) \n",
    "        dLdW = np.dot(X, np.dot(self.W1,e).T) \n",
    "        self.W1 = self.W1 - self.alpha*dLdW1 \n",
    "        self.W = self.W - self.alpha*dLdW \n",
    "           \n",
    "    def train(self,epochs): \n",
    "        for x in range(1,epochs):         \n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)): \n",
    "                self.feed_forward(self.X_train[j]) \n",
    "                self.backpropagate(self.X_train[j],self.y_train[j]) \n",
    "                C = 0\n",
    "                for m in range(self.V): \n",
    "                    if(self.y_train[j][m]): \n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u))) \n",
    "            print(\"epoch \",x, \" loss = \",self.loss) \n",
    "            self.alpha *= 1/( (1+self.alpha*x) ) \n",
    "              \n",
    "    def predict(self,word,number_of_predictions): \n",
    "        if word in self.words: \n",
    "            index = self.word_index[word] \n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X) \n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i \n",
    "               \n",
    "            top_context_words = [] \n",
    "            for k in sorted(output,reverse=True): \n",
    "                top_context_words.append(self.words[output[k]]) \n",
    "                if(len(top_context_words)>=number_of_predictions): \n",
    "                    break\n",
    "       \n",
    "            return top_context_words \n",
    "        else: \n",
    "            print(\"Word not found in dicitonary\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus): \n",
    "    stop_words = set(stopwords.words('english'))     \n",
    "    training_data = [] \n",
    "    sentences = corpus.split(\".\") \n",
    "    for i in range(len(sentences)): \n",
    "        sentences[i] = sentences[i].strip() \n",
    "        sentence = sentences[i].split() \n",
    "        x = [word.strip(string.punctuation) for word in sentence \n",
    "                                     if word not in stop_words] \n",
    "        x = [word.lower() for word in x] \n",
    "        training_data.append(x) \n",
    "    return training_data \n",
    "       \n",
    "def prepare_data_for_training(sentences,w2v): \n",
    "    data = {} \n",
    "    for sentence in sentences: \n",
    "        for word in sentence: \n",
    "            if word not in data: \n",
    "                data[word] = 1\n",
    "            else: \n",
    "                data[word] += 1\n",
    "    V = len(data) \n",
    "    data = sorted(list(data.keys())) \n",
    "    vocab = {} \n",
    "    for i in range(len(data)): \n",
    "        vocab[data[i]] = i \n",
    "       \n",
    "    #for i in range(len(words)): \n",
    "    for sentence in sentences: \n",
    "        for i in range(len(sentence)): \n",
    "            center_word = [0 for x in range(V)] \n",
    "            center_word[vocab[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)] \n",
    "              \n",
    "            for j in range(i-w2v.window_size,i+w2v.window_size): \n",
    "                if i!=j and j>=0 and j<len(sentence): \n",
    "                    context[vocab[sentence[j]]] += 1\n",
    "            w2v.X_train.append(center_word) \n",
    "            w2v.y_train.append(context) \n",
    "    w2v.initialize(V,data) \n",
    "   \n",
    "    return w2v.X_train,w2v.y_train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1  loss =  43.11740653874423\n",
      "epoch  2  loss =  43.05406908821119\n",
      "epoch  3  loss =  42.991144542913844\n",
      "epoch  4  loss =  42.92868997262881\n",
      "epoch  5  loss =  42.86676044373362\n",
      "epoch  6  loss =  42.80540873078305\n",
      "epoch  7  loss =  42.74468505585058\n",
      "epoch  8  loss =  42.68463685867467\n",
      "epoch  9  loss =  42.625308599935984\n",
      "epoch  10  loss =  42.56674159924365\n",
      "epoch  11  loss =  42.50897390865578\n",
      "epoch  12  loss =  42.45204022182291\n",
      "epoch  13  loss =  42.3959718181507\n",
      "epoch  14  loss =  42.34079654074732\n",
      "epoch  15  loss =  42.286538806366224\n",
      "epoch  16  loss =  42.23321964509108\n",
      "epoch  17  loss =  42.18085676713885\n",
      "epoch  18  loss =  42.129464653885194\n",
      "epoch  19  loss =  42.07905467003887\n",
      "epoch  20  loss =  42.02963519380545\n",
      "epoch  21  loss =  41.98121176187617\n",
      "epoch  22  loss =  41.933787226145995\n",
      "epoch  23  loss =  41.887361919194326\n",
      "epoch  24  loss =  41.84193382574093\n",
      "epoch  25  loss =  41.79749875750627\n",
      "epoch  26  loss =  41.75405052914887\n",
      "epoch  27  loss =  41.71158113321229\n",
      "epoch  28  loss =  41.67008091228007\n",
      "epoch  29  loss =  41.629538726803546\n",
      "epoch  30  loss =  41.58994211732498\n",
      "epoch  31  loss =  41.551277460064185\n",
      "epoch  32  loss =  41.51353011506561\n",
      "epoch  33  loss =  41.47668456631229\n",
      "epoch  34  loss =  41.44072455340216\n",
      "epoch  35  loss =  41.40563319454867\n",
      "epoch  36  loss =  41.37139310081307\n",
      "epoch  37  loss =  41.33798648160011\n",
      "epoch  38  loss =  41.305395241552425\n",
      "epoch  39  loss =  41.27360106906455\n",
      "epoch  40  loss =  41.242585516705674\n",
      "epoch  41  loss =  41.21233007389273\n",
      "epoch  42  loss =  41.18281623219437\n",
      "epoch  43  loss =  41.15402554367397\n",
      "epoch  44  loss =  41.12593967269552\n",
      "epoch  45  loss =  41.0985404416251\n",
      "epoch  46  loss =  41.071809870860704\n",
      "epoch  47  loss =  41.045730213618\n",
      "epoch  48  loss =  41.02028398588941\n",
      "epoch  49  loss =  40.995453991980284\n",
      "epoch  50  loss =  40.971223346008486\n",
      "epoch  51  loss =  40.94757548973582\n",
      "epoch  52  loss =  40.924494207078666\n",
      "epoch  53  loss =  40.90196363562517\n",
      "epoch  54  loss =  40.87996827546366\n",
      "epoch  55  loss =  40.858492995607286\n",
      "epoch  56  loss =  40.837523038277155\n",
      "epoch  57  loss =  40.81704402128721\n",
      "epoch  58  loss =  40.797041938753374\n",
      "epoch  59  loss =  40.77750316033144\n",
      "epoch  60  loss =  40.758414429169285\n",
      "epoch  61  loss =  40.73976285874356\n",
      "epoch  62  loss =  40.7215359287335\n",
      "epoch  63  loss =  40.703721480071415\n",
      "epoch  64  loss =  40.686307709294155\n",
      "epoch  65  loss =  40.66928316230848\n",
      "epoch  66  loss =  40.652636727670654\n",
      "epoch  67  loss =  40.63635762947029\n",
      "epoch  68  loss =  40.62043541989846\n",
      "epoch  69  loss =  40.60485997157114\n",
      "epoch  70  loss =  40.58962146967088\n",
      "epoch  71  loss =  40.57471040396214\n",
      "epoch  72  loss =  40.56011756072908\n",
      "epoch  73  loss =  40.54583401467813\n",
      "epoch  74  loss =  40.53185112084297\n",
      "epoch  75  loss =  40.51816050652344\n",
      "epoch  76  loss =  40.504754063286754\n",
      "epoch  77  loss =  40.491623939054364\n",
      "epoch  78  loss =  40.47876253029469\n",
      "epoch  79  loss =  40.46616247433908\n",
      "epoch  80  loss =  40.45381664183473\n",
      "epoch  81  loss =  40.44171812934668\n",
      "epoch  82  loss =  40.429860252117955\n",
      "epoch  83  loss =  40.41823653699585\n",
      "epoch  84  loss =  40.40684071552969\n",
      "epoch  85  loss =  40.3956667172446\n",
      "epoch  86  loss =  40.38470866309399\n",
      "epoch  87  loss =  40.373960859092854\n",
      "epoch  88  loss =  40.36341779013234\n",
      "epoch  89  loss =  40.35307411397565\n",
      "epoch  90  loss =  40.34292465543429\n",
      "epoch  91  loss =  40.33296440072323\n",
      "epoch  92  loss =  40.323188491993\n",
      "epoch  93  loss =  40.31359222203596\n",
      "epoch  94  loss =  40.304171029164046\n",
      "epoch  95  loss =  40.29492049225475\n",
      "epoch  96  loss =  40.2858363259616\n",
      "epoch  97  loss =  40.27691437608584\n",
      "epoch  98  loss =  40.26815061510503\n",
      "epoch  99  loss =  40.25954113785483\n",
      "['the', 'around', 'revolves']\n"
     ]
    }
   ],
   "source": [
    "corpus = \"The earth revolves around the sun. The moon revolves around the earth\"\n",
    "epochs = 100\n",
    "  \n",
    "training_data = preprocessing(corpus) \n",
    "w2v = word2vec() \n",
    "  \n",
    "prepare_data_for_training(training_data,w2v) \n",
    "w2v.train(epochs)  \n",
    "  \n",
    "print(w2v.predict(\"earth\",3))     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
